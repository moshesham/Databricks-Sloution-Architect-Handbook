{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - DataFrame Basics\n",
    "\n",
    "This notebook provides a hands-on introduction to PySpark DataFrames. DataFrames are a distributed collection of data organized into named columns. They are conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed (uncomment to run)\n",
    "# !pip install pyspark\n",
    "# !pip install findspark\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, avg\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames\n",
    "\n",
    "There are several ways to create DataFrames in PySpark:\n",
    "\n",
    "- From a list of rows\n",
    "- From an RDD\n",
    "- From an external data source (e.g., CSV, JSON, Parquet)\n",
    "\n",
    "### 2.1 Creating a DataFrame from a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data using Python lists\n",
    "data = [\n",
    "    (\"James\", \"\", \"Smith\", \"1991-04-01\", \"M\", 3000),\n",
    "    (\"Michael\", \"Rose\", \"\", \"2000-05-19\", \"M\", 4000),\n",
    "    (\"Robert\", \"\", \"Williams\", \"1978-09-05\", \"M\", 4000),\n",
    "    (\"Maria\", \"Anne\", \"Jones\", \"1967-12-01\", \"F\", 4000),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", \"1980-02-17\", \"F\", -1),\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data, schema=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating a DataFrame from an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from the data list\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df_from_rdd = spark.createDataFrame(rdd, schema=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Creating a DataFrame from a CSV file (External Data Source)\n",
    "\n",
    "Download the California Housing Prices dataset from Kaggle:\n",
    "https://www.kaggle.com/datasets/camnugent/california-housing-prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to your file location\n",
    "file_path = \"housing.csv\"\n",
    "\n",
    "# Create a DataFrame from the CSV file\n",
    "# df_housing = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations\n",
    "\n",
    "### 3.1 `printSchema()`\n",
    "\n",
    "Displays the schema of the DataFrame (column names and data types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 `show()`\n",
    "\n",
    "Displays the first 20 rows of the DataFrame by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 `select()`\n",
    "\n",
    "Selects specific columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstname\", \"lastname\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 `filter()`\n",
    "\n",
    "Filters rows based on a condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.salary >= 4000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 `withColumn()`\n",
    "\n",
    "Adds a new column or replaces an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"salary_increased\", col(\"salary\") * lit(1.1))  # Increase salary by 10%\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 `withColumnRenamed()`\n",
    "\n",
    "Renames an existing column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"salary_increased\", \"new_salary\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Null Values\n",
    "\n",
    "### 4.1 Identifying Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"middlename\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Filling Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = df.na.fill(\"Unknown\", subset=[\"middlename\"])\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Dropping Rows with Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.na.drop(subset=[\"middlename\"])\n",
    "df_dropped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conditional Logic with `when()` and `otherwise()`\n",
    "\n",
    "Create new columns based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"salary_grade\",\n",
    "    when(col(\"new_salary\") >= 4000, \"High\")\n",
    "    .when(col(\"new_salary\") < 4000, \"Medium\")\n",
    "    .otherwise(\"Low\"),\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This notebook covered the basics of creating and manipulating PySpark DataFrames, including:\n",
    "\n",
    "- Creating DataFrames from various sources\n",
    "- Performing basic operations like `printSchema()`, `show()`, `select()`, `filter()`, `withColumn()`, and `withColumnRenamed()`\n",
    "- Handling null values\n",
    "- Using conditional logic with `when()` and `otherwise()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

